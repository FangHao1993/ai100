MAC : 進Ａnaconda虛擬環境
    source ~/.bash_profile
    source activate test
    
資料視覺化 : D3.js

MAE : 將兩個陣列相減後, 取絕對值(abs), 再將整個陣列加總成一個數字(sum), 最後除以y的長度(len), 因此稱為"平均絕對誤差"
MAE = sum(abs(y - yp)) / len(y)

MSE : 均方誤差(Mean square error，MSE)
MSE = sum((y-yp)**2)/len(y)

機器學習是甚麼?
白話文 : 讓機器從資料中找尋規律與趨勢而不需要給定特殊規則
數學 : 給定目標函數與訓練資料，學習出能讓目標函數最佳的模型參數

EDA = Exploratory Data Analysis = '探索式資料分析'
數據分析流程 = (資料收集 - 數據清理 - 特徵萃取 - 資料視覺化 - 建立模型 - 驗證模型 - 決策應用)

資料原來是字串/類別的話，如果要做進⼀步的分析時（如訓練模型），⼀般需要轉為數值的資料類型，轉換的⽅式通常有兩種
• Label encoding：使⽤時機通常是該資料的不同類別是有序的，例如該資料是年齡分組，類別有⼩孩、年輕⼈、老⼈，表⽰為 0, 1, 2 是合理的，因為年齡上老⼈ > 年輕⼈、年輕⼈ > ⼩孩
• One Hot encoding：使⽤時機通常是該資料的不同類別是無序的，例如國家

資料特徵
    數值型特徵：最容易易轉成特徵，但需要注意很多細節
    類別型特徵：通常⼀一種類別對應⼀一種分數，問題在如何對應
    時間型特徵：特殊之處在於有週期性

Normalization(正規化)
Regularization(正規化)
    將資料在保持原始的樣態下縮放入 [0,1] 區間中。
    MinMaxScaler().fit_transform(df)
    MinMaxScaler：歸一到 [ 0，1 ] 
    MaxAbsScaler：歸一到 [ -1，1 ] 

過擬合
    增加資料量
    降低模型複雜度•
    使⽤用正規化 (Regularization)
欠擬合
    增加模型複雜度
    減輕或不使⽤正規化


Standardization(標準化)
    將原始資料轉換成符合標準常態分佈的樣態( 平均值=0、標準差=1 )。
    Z轉換 = (x-np.mean(x)) / np.std(x)
    空間轉匯
        Y = 0 ~ 1 , (x-min(x)) / (max(x) - min(x))
        Y = -1 ~ 1 , (((x-min(x)) / (max(x) - min(x))) - 0.5) * 2
        Y = 0 ~ 1 , (針對特別影像) , x/255
    非樹狀模型 : 如線性迴歸, 羅吉斯迴歸, 類神經...等，標準化 / 最⼩最⼤化後對預測'會有影響'
    樹狀模型: 如決策樹, 隨機森林林, 梯度提升樹...等，標準化 / 最⼩最大化後對預測'不會有影響'
    標準化 / 最小最大化使用上的差異
    標準化 : 轉換不易受到極端值影響
    最⼩最⼤化 : 轉換容易受到極端值影響
    註 : 因此，去過離群值的特徵，比較適⽤最⼤最小化
對數去偏(log1p)
    對數去偏就是使⽤自然對數去除偏態常見於計數 / 價格這類非負且可能為 0 的欄位因為需要將 0 對應到 0，所以先加⼀ (plus one) 再取對數 (log)還原時使⽤ expm1，也就是先取指數 (exp) 後再減⼀ (minus one)

方根去偏(sqrt) / 分布去偏(boxcox)
    方根去偏(sqrt) 就是將數值減去最⼩值後開根號，最大值有限時適⽤ (例例 : 成績轉換)
    分布去偏(boxcox)是採⽤boxcox轉換函數，函數的 lambda(λ) 參數為 0 時等於 log 函數，lambda(λ) 為 0.5 時等於開根號 (即sqrt)，因此可藉由參數的調整更靈活地轉換數值，但要特別注意Y的輸入數值必須要為正 (不可為0)

特徵選擇()
    過濾法 (Filter) : 選定統計數值與設定門檻，刪除低於門檻的特徵
    包裝法 (Wrapper) : 根據⽬標函數，逐步加入特徵或刪除特徵
    嵌入法 (Embedded) : 使用機器學習模型，根據擬合後的係數，刪除係數低於⾨檻的特徵

模型的泛化能力 (generalization)
    泛化能力越高代表模型在沒看過的測試資料上的表現能夠越好
    機器學習算法對新鮮樣本的適應能力
分類問題與回歸問題分別可用的目標函數()
    分類問題可使用交叉商 (Cross Entropy)、(AUC)、()。
    回歸問題可使用均方差 (Mean Square Error)、(R Square)
    但如果有特別希望哪⼀類別不要分錯，則可使用 F1-Score，觀察 Recall 值或是Precision 值。若是多分類問題，則可使用 top-k accuracy，k 代表模型預測前 k 個類別有包含正確類別即為正確 (ImageNet 競賽通常都是比Top-5 Accuracy)

集成()
    資料面的集成(使用不同訓練資料 + 同⼀種模型，多次估計的結果合成最終預測)
        裝袋法(Bagging)
            裝袋法顧名思義，是將資料放入袋中抽取，每回合結束後全部放回袋中重抽
            再搭配弱分類器取平均/多數決結果，最有名的就是前⾯學過的隨機森林
        提升法(Boosting)
            提升法則是由之前模型的預測結果，去改變資料被抽到的權重或目標值

            將錯判資料被抽中的機率放⼤，正確的縮⼩，就是 自適應提升 (AdaBoost, Adaptive Boosting)
            如果是依照估計誤差的殘差項調整新⽬標值，則就是 梯度提升機 (Gradient Boosting Machine) 的作法，只是梯度提升機還加上⽤梯度來選擇決策樹分⽀
    模型與特徵的集成(使用同一資料 + 不同模型，合成出不同預測結果)
        混合泛化(Blending)
            其實混合泛化非常單純，就是將不同模型的預測值加權合成，權重和為 1如果取預測的平均 or ⼀⼈一票多數決(每個模型權重相同)，則又稱為 投票泛化(Voting)
            Blending 的前提是 : 個別單模效果都很好(有調參)並且模型差異大，單模要好尤其重要，如果單模效果差異太大，Blending 的效果提升就相當有限
        堆疊泛化(Stacking)



